{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "id": "iTsNK1TvkLRo",
    "outputId": "8eeb7339-f9ff-4e51-fc31-528bdcc179ce"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time, os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "z0-_Sm9EAhVL"
   },
   "outputs": [],
   "source": [
    "def plot_predicted_fn(inp, op, derv):\n",
    "    \"\"\"\n",
    "    For plotting predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.plot(inp,op)\n",
    "    plt.title(\"x vs t\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(inp, derv)\n",
    "    plt.title(\"x_t vs t\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TVo78_DCkWvh",
    "outputId": "89892783-ff05-4cbc-d0e0-303962cca3d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0\n",
      "loss :  20482914.0\n",
      "Epoch : 500\n",
      "loss :  20048194.0\n",
      "Epoch : 1000\n",
      "loss :  19434078.0\n",
      "Epoch : 1500\n",
      "loss :  18833086.0\n",
      "Epoch : 2000\n",
      "loss :  18262024.0\n",
      "Epoch : 2500\n",
      "loss :  17725684.0\n",
      "Epoch : 3000\n",
      "loss :  17226006.0\n",
      "Epoch : 3500\n",
      "loss :  16763529.0\n",
      "Epoch : 4000\n",
      "loss :  16336764.0\n",
      "Epoch : 4500\n",
      "loss :  15915875.0\n",
      "Epoch : 5000\n",
      "loss :  15499506.0\n",
      "Epoch : 5500\n",
      "loss :  15096554.0\n",
      "Epoch : 6000\n",
      "loss :  14705725.0\n",
      "Epoch : 6500\n",
      "loss :  14320896.0\n",
      "Epoch : 7000\n",
      "loss :  13948739.0\n",
      "Epoch : 7500\n",
      "loss :  13585327.0\n",
      "Epoch : 8000\n",
      "loss :  13230230.0\n",
      "Epoch : 8500\n",
      "loss :  12883009.0\n",
      "Epoch : 9000\n",
      "loss :  12542866.0\n",
      "Epoch : 9500\n",
      "loss :  12209801.0\n",
      "Epoch : 10000\n",
      "loss :  11883992.0\n",
      "Epoch : 10500\n",
      "loss :  11565391.0\n",
      "Epoch : 11000\n",
      "loss :  11252856.0\n",
      "Epoch : 11500\n",
      "loss :  10947174.0\n",
      "Epoch : 12000\n",
      "loss :  10647580.0\n",
      "Epoch : 12500\n",
      "loss :  10354687.0\n",
      "Epoch : 13000\n",
      "loss :  10066858.0\n",
      "Epoch : 13500\n",
      "loss :  9786250.0\n",
      "Epoch : 14000\n",
      "loss :  9509137.0\n",
      "Epoch : 14500\n",
      "loss :  9238406.0\n",
      "Epoch : 15000\n",
      "loss :  8974266.0\n",
      "Epoch : 15500\n",
      "loss :  8713287.0\n",
      "Epoch : 16000\n",
      "loss :  8459094.0\n",
      "Epoch : 16500\n",
      "loss :  8209246.5\n",
      "Epoch : 17000\n",
      "loss :  7964608.0\n",
      "Epoch : 17500\n",
      "loss :  7725499.0\n",
      "Epoch : 18000\n",
      "loss :  7492174.0\n",
      "Epoch : 18500\n",
      "loss :  7270759.0\n",
      "Epoch : 19000\n",
      "loss :  7036422.5\n",
      "Epoch : 19500\n",
      "loss :  6816420.5\n",
      "learning rate: 0.001\n",
      "Epoch : 20000\n",
      "loss :  6601240.5\n",
      "Epoch : 20500\n",
      "loss :  6390788.5\n",
      "Epoch : 21000\n",
      "loss :  6183343.5\n",
      "Epoch : 21500\n",
      "loss :  5980512.0\n",
      "Epoch : 22000\n",
      "loss :  5782089.5\n",
      "Epoch : 22500\n",
      "loss :  5588487.0\n",
      "Epoch : 23000\n",
      "loss :  5399177.5\n",
      "Epoch : 23500\n",
      "loss :  5213377.5\n",
      "Epoch : 24000\n",
      "loss :  5031454.0\n",
      "Epoch : 24500\n",
      "loss :  4854411.0\n",
      "Epoch : 25000\n",
      "loss :  4681469.0\n",
      "Epoch : 25500\n",
      "loss :  4511578.0\n",
      "Epoch : 26000\n",
      "loss :  4350277.0\n",
      "Epoch : 26500\n",
      "loss :  4185458.0\n",
      "Epoch : 27000\n",
      "loss :  4026561.2\n",
      "Epoch : 27500\n",
      "loss :  3872181.8\n",
      "Epoch : 28000\n",
      "loss :  3723059.5\n",
      "Epoch : 28500\n",
      "loss :  3574989.5\n",
      "Epoch : 29000\n",
      "loss :  3432827.5\n",
      "Epoch : 29500\n",
      "loss :  3291643.8\n",
      "Epoch : 30000\n",
      "loss :  3154918.5\n",
      "Epoch : 30500\n",
      "loss :  3022734.0\n",
      "Epoch : 31000\n",
      "loss :  2893716.8\n",
      "Epoch : 31500\n",
      "loss :  2767525.8\n",
      "Epoch : 32000\n",
      "loss :  2645095.8\n",
      "Epoch : 32500\n",
      "loss :  2527682.8\n",
      "Epoch : 33000\n",
      "loss :  2409948.2\n",
      "Epoch : 33500\n",
      "loss :  2297045.8\n",
      "Epoch : 34000\n",
      "loss :  2192784.5\n",
      "Epoch : 34500\n",
      "loss :  2081237.1\n",
      "Epoch : 35000\n",
      "loss :  1978188.0\n",
      "Epoch : 35500\n",
      "loss :  1880307.9\n",
      "Epoch : 36000\n",
      "loss :  1783161.0\n",
      "Epoch : 36500\n",
      "loss :  1687279.4\n",
      "Epoch : 37000\n",
      "loss :  1596876.5\n",
      "Epoch : 37500\n",
      "loss :  1508296.0\n",
      "Epoch : 38000\n",
      "loss :  1423333.6\n",
      "Epoch : 38500\n",
      "loss :  1341085.5\n",
      "Epoch : 39000\n",
      "loss :  1261941.8\n",
      "Epoch : 39500\n",
      "loss :  1185659.2\n",
      "learning rate: 0.0001\n",
      "Epoch : 40000\n",
      "loss :  1111948.4\n",
      "Epoch : 40500\n",
      "loss :  1049817.5\n",
      "Epoch : 41000\n",
      "loss :  972375.4\n",
      "Epoch : 41500\n",
      "loss :  907054.56\n",
      "Epoch : 42000\n",
      "loss :  847266.25\n",
      "Epoch : 42500\n",
      "loss :  784186.94\n",
      "Epoch : 43000\n",
      "loss :  726689.94\n",
      "Epoch : 43500\n",
      "loss :  671817.1\n",
      "Epoch : 44000\n",
      "loss :  620349.8\n",
      "Epoch : 44500\n",
      "loss :  570801.44\n",
      "Epoch : 45000\n",
      "loss :  521253.97\n",
      "Epoch : 45500\n",
      "loss :  476779.03\n",
      "Epoch : 46000\n",
      "loss :  433181.9\n",
      "Epoch : 46500\n",
      "loss :  395774.16\n",
      "Epoch : 47000\n",
      "loss :  354616.75\n",
      "Epoch : 47500\n",
      "loss :  319711.25\n",
      "Epoch : 48000\n",
      "loss :  285461.3\n",
      "Epoch : 48500\n",
      "loss :  255328.34\n",
      "Epoch : 49000\n",
      "loss :  227045.19\n",
      "Epoch : 49500\n",
      "loss :  196826.08\n",
      "Epoch : 50000\n",
      "loss :  172206.5\n",
      "Epoch : 50500\n",
      "loss :  153926.56\n",
      "Epoch : 51000\n",
      "loss :  127860.01\n",
      "Epoch : 51500\n",
      "loss :  108079.66\n",
      "Epoch : 52000\n",
      "loss :  90927.65\n",
      "Epoch : 52500\n",
      "loss :  75311.56\n",
      "Epoch : 53000\n",
      "loss :  62212.42\n",
      "Epoch : 53500\n",
      "loss :  49841.63\n",
      "Epoch : 54000\n",
      "loss :  39289.44\n",
      "Epoch : 54500\n",
      "loss :  30457.125\n",
      "Epoch : 55000\n",
      "loss :  24181.223\n",
      "Epoch : 55500\n",
      "loss :  18671.123\n",
      "Epoch : 56000\n",
      "loss :  11786.768\n",
      "Epoch : 56500\n",
      "loss :  8540.5\n",
      "Epoch : 57000\n",
      "loss :  15144.07\n",
      "Epoch : 57500\n",
      "loss :  3052.653\n",
      "Epoch : 58000\n",
      "loss :  1764.0479\n",
      "Epoch : 58500\n",
      "loss :  847.6262\n",
      "Epoch : 59000\n",
      "loss :  2988.6108\n",
      "Epoch : 59500\n",
      "loss :  2780.7832\n",
      "learning rate: 1e-05\n",
      "Epoch : 60000\n",
      "loss :  93.95902\n",
      "Epoch : 60500\n",
      "loss :  2065.4053\n",
      "Epoch : 61000\n",
      "loss :  3562.2388\n",
      "Epoch : 61500\n",
      "loss :  9788.655\n",
      "Epoch : 62000\n",
      "loss :  1246.9617\n",
      "Epoch : 62500\n",
      "loss :  33.880783\n",
      "Epoch : 63000\n",
      "loss :  102.57705\n",
      "Epoch : 63500\n",
      "loss :  3262.3403\n",
      "Epoch : 64000\n",
      "loss :  148.97559\n",
      "Epoch : 64500\n",
      "loss :  148.53313\n",
      "Epoch : 65000\n",
      "loss :  233.49734\n",
      "Epoch : 65500\n",
      "loss :  63.21089\n",
      "Epoch : 66000\n",
      "loss :  7782.5386\n",
      "Epoch : 66500\n",
      "loss :  375.97116\n",
      "Epoch : 67000\n",
      "loss :  1364.1362\n",
      "Epoch : 67500\n",
      "loss :  76.24361\n",
      "Epoch : 68000\n",
      "loss :  1062.5519\n",
      "Epoch : 68500\n",
      "loss :  92.5172\n",
      "Epoch : 69000\n",
      "loss :  150.57278\n",
      "Epoch : 69500\n",
      "loss :  92.57627\n",
      "Epoch : 70000\n",
      "loss :  99.79572\n",
      "Epoch : 70500\n",
      "loss :  26.135931\n",
      "Epoch : 71000\n",
      "loss :  16.121655\n",
      "Epoch : 71500\n",
      "loss :  95.51651\n",
      "Epoch : 72000\n",
      "loss :  357.37796\n",
      "Epoch : 72500\n",
      "loss :  5.334217\n",
      "Epoch : 73000\n",
      "loss :  123.69309\n",
      "Epoch : 73500\n",
      "loss :  80.94725\n",
      "Epoch : 74000\n",
      "loss :  463.21457\n",
      "Epoch : 74500\n",
      "loss :  199.78842\n",
      "Epoch : 75000\n",
      "loss :  12.107713\n",
      "Epoch : 75500\n",
      "loss :  367.00375\n",
      "Epoch : 76000\n",
      "loss :  10.549339\n",
      "Epoch : 76500\n",
      "loss :  343.69333\n",
      "Epoch : 77000\n",
      "loss :  3634.8374\n",
      "Epoch : 77500\n",
      "loss :  1278.4438\n",
      "Epoch : 78000\n",
      "loss :  1932.5464\n",
      "Epoch : 78500\n",
      "loss :  69.25941\n",
      "Epoch : 79000\n",
      "loss :  3177.242\n",
      "Epoch : 79500\n",
      "loss :  124.98867\n",
      "learning rate: 1.0000000000000002e-06\n",
      "Epoch : 80000\n",
      "loss :  2797.3813\n",
      "Epoch : 80500\n",
      "loss :  304.10425\n",
      "Epoch : 81000\n",
      "loss :  83.968956\n",
      "Epoch : 81500\n",
      "loss :  5258.8843\n",
      "Epoch : 82000\n",
      "loss :  237.97432\n",
      "Epoch : 82500\n",
      "loss :  75.08989\n",
      "Epoch : 83000\n",
      "loss :  5348.997\n",
      "Epoch : 83500\n",
      "loss :  4157.495\n",
      "Epoch : 84000\n",
      "loss :  46.828735\n",
      "Epoch : 84500\n",
      "loss :  37.4122\n",
      "Epoch : 85000\n",
      "loss :  1.0082848\n",
      "Epoch : 85500\n",
      "loss :  3996.5044\n",
      "Epoch : 86000\n",
      "loss :  3189.346\n",
      "Epoch : 86500\n",
      "loss :  919.35876\n",
      "Epoch : 87000\n",
      "loss :  1960.7365\n",
      "Epoch : 87500\n",
      "loss :  24.925383\n",
      "Epoch : 88000\n",
      "loss :  138.05574\n",
      "Epoch : 88500\n",
      "loss :  3166.8994\n",
      "Epoch : 89000\n",
      "loss :  4435.439\n",
      "Epoch : 89500\n",
      "loss :  595.39594\n",
      "Epoch : 90000\n",
      "loss :  10.694837\n",
      "Epoch : 90500\n",
      "loss :  1.2810022\n",
      "Epoch : 91000\n",
      "loss :  90.50946\n",
      "Epoch : 91500\n",
      "loss :  6525.881\n",
      "Epoch : 92000\n",
      "loss :  57.082714\n",
      "Epoch : 92500\n",
      "loss :  3.1183546\n",
      "Epoch : 93000\n",
      "loss :  1.5943823\n",
      "Epoch : 93500\n",
      "loss :  18.044727\n",
      "Epoch : 94000\n",
      "loss :  20.352806\n",
      "Epoch : 94500\n",
      "loss :  6.7150145\n",
      "Epoch : 95000\n",
      "loss :  289.16583\n",
      "Epoch : 95500\n",
      "loss :  14568.697\n",
      "Epoch : 96000\n",
      "loss :  205.95465\n",
      "Epoch : 96500\n",
      "loss :  4625.172\n",
      "Epoch : 97000\n",
      "loss :  26.296192\n",
      "Epoch : 97500\n",
      "loss :  597.39764\n",
      "Epoch : 98000\n",
      "loss :  100.04914\n",
      "Epoch : 98500\n",
      "loss :  366.9129\n",
      "Epoch : 99000\n",
      "loss :  5.9080987\n",
      "Epoch : 99500\n",
      "loss :  1597.7319\n",
      "Input :  [[  1]\n",
      " [  2]\n",
      " [  3]\n",
      " [  4]\n",
      " [  5]\n",
      " [  6]\n",
      " [  7]\n",
      " [  8]\n",
      " [  9]\n",
      " [ 10]\n",
      " [ 11]\n",
      " [ 12]\n",
      " [ 13]\n",
      " [ 14]\n",
      " [ 15]\n",
      " [ 16]\n",
      " [ 17]\n",
      " [ 18]\n",
      " [ 19]\n",
      " [ 20]\n",
      " [ 21]\n",
      " [ 22]\n",
      " [ 23]\n",
      " [ 24]\n",
      " [ 25]\n",
      " [ 26]\n",
      " [ 27]\n",
      " [ 28]\n",
      " [ 29]\n",
      " [ 30]\n",
      " [ 31]\n",
      " [ 32]\n",
      " [ 33]\n",
      " [ 34]\n",
      " [ 35]\n",
      " [ 36]\n",
      " [ 37]\n",
      " [ 38]\n",
      " [ 39]\n",
      " [ 40]\n",
      " [ 41]\n",
      " [ 42]\n",
      " [ 43]\n",
      " [ 44]\n",
      " [ 45]\n",
      " [ 46]\n",
      " [ 47]\n",
      " [ 48]\n",
      " [ 49]\n",
      " [ 50]\n",
      " [ 51]\n",
      " [ 52]\n",
      " [ 53]\n",
      " [ 54]\n",
      " [ 55]\n",
      " [ 56]\n",
      " [ 57]\n",
      " [ 58]\n",
      " [ 59]\n",
      " [ 60]\n",
      " [ 61]\n",
      " [ 62]\n",
      " [ 63]\n",
      " [ 64]\n",
      " [ 65]\n",
      " [ 66]\n",
      " [ 67]\n",
      " [ 68]\n",
      " [ 69]\n",
      " [ 70]\n",
      " [ 71]\n",
      " [ 72]\n",
      " [ 73]\n",
      " [ 74]\n",
      " [ 75]\n",
      " [ 76]\n",
      " [ 77]\n",
      " [ 78]\n",
      " [ 79]\n",
      " [ 80]\n",
      " [ 81]\n",
      " [ 82]\n",
      " [ 83]\n",
      " [ 84]\n",
      " [ 85]\n",
      " [ 86]\n",
      " [ 87]\n",
      " [ 88]\n",
      " [ 89]\n",
      " [ 90]\n",
      " [ 91]\n",
      " [ 92]\n",
      " [ 93]\n",
      " [ 94]\n",
      " [ 95]\n",
      " [ 96]\n",
      " [ 97]\n",
      " [ 98]\n",
      " [ 99]\n",
      " [100]] output :  [[-171774.28  ]\n",
      " [-171769.89  ]\n",
      " [-171762.97  ]\n",
      " [-171752.19  ]\n",
      " [-171735.53  ]\n",
      " [-171710.38  ]\n",
      " [-171673.38  ]\n",
      " [-171621.16  ]\n",
      " [-171551.    ]\n",
      " [-171461.9   ]\n",
      " [-171353.94  ]\n",
      " [-171225.86  ]\n",
      " [-171073.38  ]\n",
      " [-170892.84  ]\n",
      " [-170685.67  ]\n",
      " [-170449.42  ]\n",
      " [-170178.2   ]\n",
      " [-169878.52  ]\n",
      " [-169530.72  ]\n",
      " [-169170.9   ]\n",
      " [-168733.95  ]\n",
      " [-168264.6   ]\n",
      " [-167787.4   ]\n",
      " [-167223.61  ]\n",
      " [-166642.22  ]\n",
      " [-165980.4   ]\n",
      " [-165288.    ]\n",
      " [-164538.44  ]\n",
      " [-163721.2   ]\n",
      " [-162857.56  ]\n",
      " [-161929.78  ]\n",
      " [-160940.94  ]\n",
      " [-159888.34  ]\n",
      " [-158767.62  ]\n",
      " [-157580.5   ]\n",
      " [-156323.66  ]\n",
      " [-154994.6   ]\n",
      " [-153590.47  ]\n",
      " [-152111.45  ]\n",
      " [-150553.94  ]\n",
      " [-148917.06  ]\n",
      " [-147197.47  ]\n",
      " [-145393.92  ]\n",
      " [-143504.92  ]\n",
      " [-141527.25  ]\n",
      " [-139461.6   ]\n",
      " [-137293.56  ]\n",
      " [-135076.69  ]\n",
      " [-132626.11  ]\n",
      " [-130220.42  ]\n",
      " [-127661.375 ]\n",
      " [-125015.914 ]\n",
      " [-122260.516 ]\n",
      " [-119403.27  ]\n",
      " [-116433.03  ]\n",
      " [-113359.64  ]\n",
      " [-110166.69  ]\n",
      " [-106864.72  ]\n",
      " [-103445.1   ]\n",
      " [ -99907.945 ]\n",
      " [ -96250.3   ]\n",
      " [ -92471.36  ]\n",
      " [ -88567.76  ]\n",
      " [ -84538.53  ]\n",
      " [ -80381.234 ]\n",
      " [ -76093.805 ]\n",
      " [ -71674.516 ]\n",
      " [ -67121.12  ]\n",
      " [ -62431.777 ]\n",
      " [ -57604.387 ]\n",
      " [ -52637.062 ]\n",
      " [ -47527.676 ]\n",
      " [ -42274.406 ]\n",
      " [ -36875.016 ]\n",
      " [ -31327.67  ]\n",
      " [ -25630.467 ]\n",
      " [ -19780.902 ]\n",
      " [ -13777.769 ]\n",
      " [  -7618.2695]\n",
      " [  -1300.9202]\n",
      " [   5176.381 ]\n",
      " [  11815.824 ]\n",
      " [  18619.082 ]\n",
      " [  25588.43  ]\n",
      " [  32725.672 ]\n",
      " [  40033.043 ]\n",
      " [  47512.17  ]\n",
      " [  55165.74  ]\n",
      " [  62994.79  ]\n",
      " [  71002.53  ]\n",
      " [  79189.58  ]\n",
      " [  87559.7   ]\n",
      " [  96111.8   ]\n",
      " [ 104853.79  ]\n",
      " [ 113776.01  ]\n",
      " [ 122903.92  ]\n",
      " [ 132191.53  ]\n",
      " [ 141731.98  ]\n",
      " [ 151368.75  ]\n",
      " [ 161416.42  ]] loss :  0.85376185\n",
      "Derivative:  [[3.46124268e+00]\n",
      " [5.46730089e+00]\n",
      " [8.58149719e+00]\n",
      " [1.33347178e+01]\n",
      " [2.04007092e+01]\n",
      " [3.04905910e+01]\n",
      " [4.40646400e+01]\n",
      " [6.08634109e+01]\n",
      " [7.95904846e+01]\n",
      " [9.85388412e+01]\n",
      " [1.17558716e+02]\n",
      " [1.39387390e+02]\n",
      " [1.66315308e+02]\n",
      " [1.94318268e+02]\n",
      " [2.20288666e+02]\n",
      " [2.54035461e+02]\n",
      " [2.85468323e+02]\n",
      " [3.21144165e+02]\n",
      " [3.57942413e+02]\n",
      " [3.97034149e+02]\n",
      " [4.38007812e+02]\n",
      " [4.81020996e+02]\n",
      " [5.26007812e+02]\n",
      " [5.72997864e+02]\n",
      " [6.22003052e+02]\n",
      " [6.73020569e+02]\n",
      " [7.26021606e+02]\n",
      " [7.81017944e+02]\n",
      " [8.37991821e+02]\n",
      " [8.96990356e+02]\n",
      " [9.58003296e+02]\n",
      " [1.02102313e+03]\n",
      " [1.08603003e+03]\n",
      " [1.15302771e+03]\n",
      " [1.22201245e+03]\n",
      " [1.29299194e+03]\n",
      " [1.36598926e+03]\n",
      " [1.44100098e+03]\n",
      " [1.51801221e+03]\n",
      " [1.59702588e+03]\n",
      " [1.67803711e+03]\n",
      " [1.76103418e+03]\n",
      " [1.84601611e+03]\n",
      " [1.93299646e+03]\n",
      " [2.02198486e+03]\n",
      " [2.11298242e+03]\n",
      " [2.20598828e+03]\n",
      " [2.30100439e+03]\n",
      " [2.39802783e+03]\n",
      " [2.49702759e+03]\n",
      " [2.59803125e+03]\n",
      " [2.70101221e+03]\n",
      " [2.80603613e+03]\n",
      " [2.91298608e+03]\n",
      " [3.02201636e+03]\n",
      " [3.13300171e+03]\n",
      " [3.24592090e+03]\n",
      " [3.36122437e+03]\n",
      " [3.47761475e+03]\n",
      " [3.59740430e+03]\n",
      " [3.71781226e+03]\n",
      " [3.84094531e+03]\n",
      " [3.96627148e+03]\n",
      " [4.09275391e+03]\n",
      " [4.22220215e+03]\n",
      " [4.35292480e+03]\n",
      " [4.48606348e+03]\n",
      " [4.62099609e+03]\n",
      " [4.75804395e+03]\n",
      " [4.89700977e+03]\n",
      " [5.03803076e+03]\n",
      " [5.18101953e+03]\n",
      " [5.32593750e+03]\n",
      " [5.47315820e+03]\n",
      " [5.62181885e+03]\n",
      " [5.77308887e+03]\n",
      " [5.92606543e+03]\n",
      " [6.08072705e+03]\n",
      " [6.23831641e+03]\n",
      " [6.39667969e+03]\n",
      " [6.55822266e+03]\n",
      " [6.72093896e+03]\n",
      " [6.88606641e+03]\n",
      " [7.05311035e+03]\n",
      " [7.22194385e+03]\n",
      " [7.39325098e+03]\n",
      " [7.56571143e+03]\n",
      " [7.74141797e+03]\n",
      " [7.91749170e+03]\n",
      " [8.09739062e+03]\n",
      " [8.27745996e+03]\n",
      " [8.46128125e+03]\n",
      " [8.64568555e+03]\n",
      " [8.83318359e+03]\n",
      " [9.02196484e+03]\n",
      " [9.21323828e+03]\n",
      " [9.40611230e+03]\n",
      " [9.60109766e+03]\n",
      " [9.79789551e+03]\n",
      " [9.99696387e+03]]\n",
      "learning rate: 1.0000000000000002e-07\n",
      "Input :  [[  1]\n",
      " [  2]\n",
      " [  3]\n",
      " [  4]\n",
      " [  5]\n",
      " [  6]\n",
      " [  7]\n",
      " [  8]\n",
      " [  9]\n",
      " [ 10]\n",
      " [ 11]\n",
      " [ 12]\n",
      " [ 13]\n",
      " [ 14]\n",
      " [ 15]\n",
      " [ 16]\n",
      " [ 17]\n",
      " [ 18]\n",
      " [ 19]\n",
      " [ 20]\n",
      " [ 21]\n",
      " [ 22]\n",
      " [ 23]\n",
      " [ 24]\n",
      " [ 25]\n",
      " [ 26]\n",
      " [ 27]\n",
      " [ 28]\n",
      " [ 29]\n",
      " [ 30]\n",
      " [ 31]\n",
      " [ 32]\n",
      " [ 33]\n",
      " [ 34]\n",
      " [ 35]\n",
      " [ 36]\n",
      " [ 37]\n",
      " [ 38]\n",
      " [ 39]\n",
      " [ 40]\n",
      " [ 41]\n",
      " [ 42]\n",
      " [ 43]\n",
      " [ 44]\n",
      " [ 45]\n",
      " [ 46]\n",
      " [ 47]\n",
      " [ 48]\n",
      " [ 49]\n",
      " [ 50]\n",
      " [ 51]\n",
      " [ 52]\n",
      " [ 53]\n",
      " [ 54]\n",
      " [ 55]\n",
      " [ 56]\n",
      " [ 57]\n",
      " [ 58]\n",
      " [ 59]\n",
      " [ 60]\n",
      " [ 61]\n",
      " [ 62]\n",
      " [ 63]\n",
      " [ 64]\n",
      " [ 65]\n",
      " [ 66]\n",
      " [ 67]\n",
      " [ 68]\n",
      " [ 69]\n",
      " [ 70]\n",
      " [ 71]\n",
      " [ 72]\n",
      " [ 73]\n",
      " [ 74]\n",
      " [ 75]\n",
      " [ 76]\n",
      " [ 77]\n",
      " [ 78]\n",
      " [ 79]\n",
      " [ 80]\n",
      " [ 81]\n",
      " [ 82]\n",
      " [ 83]\n",
      " [ 84]\n",
      " [ 85]\n",
      " [ 86]\n",
      " [ 87]\n",
      " [ 88]\n",
      " [ 89]\n",
      " [ 90]\n",
      " [ 91]\n",
      " [ 92]\n",
      " [ 93]\n",
      " [ 94]\n",
      " [ 95]\n",
      " [ 96]\n",
      " [ 97]\n",
      " [ 98]\n",
      " [ 99]\n",
      " [100]] output :  [[-171774.28  ]\n",
      " [-171769.89  ]\n",
      " [-171762.97  ]\n",
      " [-171752.19  ]\n",
      " [-171735.53  ]\n",
      " [-171710.38  ]\n",
      " [-171673.38  ]\n",
      " [-171621.16  ]\n",
      " [-171551.    ]\n",
      " [-171461.92  ]\n",
      " [-171353.94  ]\n",
      " [-171225.88  ]\n",
      " [-171073.38  ]\n",
      " [-170892.86  ]\n",
      " [-170685.67  ]\n",
      " [-170449.44  ]\n",
      " [-170178.22  ]\n",
      " [-169878.53  ]\n",
      " [-169530.72  ]\n",
      " [-169170.9   ]\n",
      " [-168733.94  ]\n",
      " [-168264.6   ]\n",
      " [-167787.4   ]\n",
      " [-167223.61  ]\n",
      " [-166642.23  ]\n",
      " [-165980.42  ]\n",
      " [-165287.97  ]\n",
      " [-164538.4   ]\n",
      " [-163721.19  ]\n",
      " [-162857.56  ]\n",
      " [-161929.81  ]\n",
      " [-160941.    ]\n",
      " [-159888.34  ]\n",
      " [-158767.6   ]\n",
      " [-157580.45  ]\n",
      " [-156323.64  ]\n",
      " [-154994.6   ]\n",
      " [-153590.52  ]\n",
      " [-152111.5   ]\n",
      " [-150553.95  ]\n",
      " [-148917.02  ]\n",
      " [-147197.4   ]\n",
      " [-145393.83  ]\n",
      " [-143504.84  ]\n",
      " [-141527.2   ]\n",
      " [-139461.61  ]\n",
      " [-137293.62  ]\n",
      " [-135076.75  ]\n",
      " [-132626.16  ]\n",
      " [-130220.42  ]\n",
      " [-127661.336 ]\n",
      " [-125015.86  ]\n",
      " [-122260.45  ]\n",
      " [-119403.22  ]\n",
      " [-116433.016 ]\n",
      " [-113359.68  ]\n",
      " [-110166.75  ]\n",
      " [-106864.81  ]\n",
      " [-103445.22  ]\n",
      " [ -99908.03  ]\n",
      " [ -96250.38  ]\n",
      " [ -92471.414 ]\n",
      " [ -88567.77  ]\n",
      " [ -84538.54  ]\n",
      " [ -80381.2   ]\n",
      " [ -76093.78  ]\n",
      " [ -71674.484 ]\n",
      " [ -67121.09  ]\n",
      " [ -62431.77  ]\n",
      " [ -57604.375 ]\n",
      " [ -52637.055 ]\n",
      " [ -47527.664 ]\n",
      " [ -42274.387 ]\n",
      " [ -36874.992 ]\n",
      " [ -31327.637 ]\n",
      " [ -25630.441 ]\n",
      " [ -19780.895 ]\n",
      " [ -13777.799 ]\n",
      " [  -7618.353 ]\n",
      " [  -1301.0557]\n",
      " [   5176.211 ]\n",
      " [  11815.665 ]\n",
      " [  18618.973 ]\n",
      " [  25588.41  ]\n",
      " [  32725.75  ]\n",
      " [  40033.223 ]\n",
      " [  47512.438 ]\n",
      " [  55166.04  ]\n",
      " [  62995.055 ]\n",
      " [  71002.65  ]\n",
      " [  79189.5   ]\n",
      " [  87559.4   ]\n",
      " [  96111.31  ]\n",
      " [ 104853.234 ]\n",
      " [ 113775.53  ]\n",
      " [ 122903.695 ]\n",
      " [ 132191.62  ]\n",
      " [ 141732.34  ]\n",
      " [ 151369.03  ]\n",
      " [ 161416.56  ]] loss :  0.8534663\n",
      "Derivative:  [[3.4612007e+00]\n",
      " [5.4675012e+00]\n",
      " [8.5816870e+00]\n",
      " [1.3335005e+01]\n",
      " [2.0401363e+01]\n",
      " [3.0491571e+01]\n",
      " [4.4065987e+01]\n",
      " [6.0865105e+01]\n",
      " [7.9592285e+01]\n",
      " [9.8540222e+01]\n",
      " [1.1755922e+02]\n",
      " [1.3938672e+02]\n",
      " [1.6631390e+02]\n",
      " [1.9431624e+02]\n",
      " [2.2028619e+02]\n",
      " [2.5403285e+02]\n",
      " [2.8546771e+02]\n",
      " [3.2114624e+02]\n",
      " [3.5794376e+02]\n",
      " [3.9703412e+02]\n",
      " [4.3800833e+02]\n",
      " [4.8100629e+02]\n",
      " [5.2600092e+02]\n",
      " [5.7302417e+02]\n",
      " [6.2202142e+02]\n",
      " [6.7300208e+02]\n",
      " [7.2599762e+02]\n",
      " [7.8100024e+02]\n",
      " [8.3802222e+02]\n",
      " [8.9702307e+02]\n",
      " [9.5801611e+02]\n",
      " [1.0210079e+03]\n",
      " [1.0859962e+03]\n",
      " [1.1529977e+03]\n",
      " [1.2220151e+03]\n",
      " [1.2930239e+03]\n",
      " [1.3660281e+03]\n",
      " [1.4410265e+03]\n",
      " [1.5180122e+03]\n",
      " [1.5969956e+03]\n",
      " [1.6779891e+03]\n",
      " [1.7609908e+03]\n",
      " [1.8459968e+03]\n",
      " [1.9330078e+03]\n",
      " [2.0220208e+03]\n",
      " [2.1130325e+03]\n",
      " [2.2060247e+03]\n",
      " [2.3010173e+03]\n",
      " [2.3980027e+03]\n",
      " [2.4970020e+03]\n",
      " [2.5980022e+03]\n",
      " [2.7009844e+03]\n",
      " [2.8060151e+03]\n",
      " [2.9129727e+03]\n",
      " [3.0220146e+03]\n",
      " [3.1330085e+03]\n",
      " [3.2459392e+03]\n",
      " [3.3612493e+03]\n",
      " [3.4776458e+03]\n",
      " [3.5974324e+03]\n",
      " [3.7178374e+03]\n",
      " [3.8409634e+03]\n",
      " [3.9662778e+03]\n",
      " [4.0927490e+03]\n",
      " [4.2221855e+03]\n",
      " [4.3528936e+03]\n",
      " [4.4860234e+03]\n",
      " [4.6209497e+03]\n",
      " [4.7579980e+03]\n",
      " [4.8969692e+03]\n",
      " [5.0380029e+03]\n",
      " [5.1810078e+03]\n",
      " [5.3259443e+03]\n",
      " [5.4731904e+03]\n",
      " [5.6218721e+03]\n",
      " [5.7731592e+03]\n",
      " [5.9261450e+03]\n",
      " [6.0808037e+03]\n",
      " [6.2383682e+03]\n",
      " [6.3966982e+03]\n",
      " [6.5581997e+03]\n",
      " [6.7208730e+03]\n",
      " [6.8859678e+03]\n",
      " [7.0529932e+03]\n",
      " [7.2218306e+03]\n",
      " [7.3931719e+03]\n",
      " [7.5656826e+03]\n",
      " [7.7414580e+03]\n",
      " [7.9176069e+03]\n",
      " [8.0975537e+03]\n",
      " [8.2776553e+03]\n",
      " [8.4614482e+03]\n",
      " [8.6457910e+03]\n",
      " [8.8331621e+03]\n",
      " [9.0218037e+03]\n",
      " [9.2129629e+03]\n",
      " [9.4058281e+03]\n",
      " [9.6010186e+03]\n",
      " [9.7980166e+03]\n",
      " [9.9971084e+03]]\n",
      "[[-171774.28  ]\n",
      " [-171769.89  ]\n",
      " [-171762.97  ]\n",
      " [-171752.19  ]\n",
      " [-171735.53  ]\n",
      " [-171710.38  ]\n",
      " [-171673.38  ]\n",
      " [-171621.16  ]\n",
      " [-171551.    ]\n",
      " [-171461.9   ]\n",
      " [-171353.94  ]\n",
      " [-171225.86  ]\n",
      " [-171073.38  ]\n",
      " [-170892.84  ]\n",
      " [-170685.66  ]\n",
      " [-170449.42  ]\n",
      " [-170178.22  ]\n",
      " [-169878.53  ]\n",
      " [-169530.72  ]\n",
      " [-169170.9   ]\n",
      " [-168733.94  ]\n",
      " [-168264.6   ]\n",
      " [-167787.42  ]\n",
      " [-167223.62  ]\n",
      " [-166642.22  ]\n",
      " [-165980.4   ]\n",
      " [-165287.98  ]\n",
      " [-164538.44  ]\n",
      " [-163721.2   ]\n",
      " [-162857.56  ]\n",
      " [-161929.78  ]\n",
      " [-160940.95  ]\n",
      " [-159888.34  ]\n",
      " [-158767.62  ]\n",
      " [-157580.48  ]\n",
      " [-156323.66  ]\n",
      " [-154994.58  ]\n",
      " [-153590.47  ]\n",
      " [-152111.44  ]\n",
      " [-150553.9   ]\n",
      " [-148917.02  ]\n",
      " [-147197.44  ]\n",
      " [-145393.9   ]\n",
      " [-143504.9   ]\n",
      " [-141527.25  ]\n",
      " [-139461.61  ]\n",
      " [-137293.6   ]\n",
      " [-135076.69  ]\n",
      " [-132626.1   ]\n",
      " [-130220.41  ]\n",
      " [-127661.34  ]\n",
      " [-125015.875 ]\n",
      " [-122260.49  ]\n",
      " [-119403.28  ]\n",
      " [-116433.086 ]\n",
      " [-113359.74  ]\n",
      " [-110166.81  ]\n",
      " [-106864.84  ]\n",
      " [-103445.23  ]\n",
      " [ -99908.02  ]\n",
      " [ -96250.336 ]\n",
      " [ -92471.34  ]\n",
      " [ -88567.695 ]\n",
      " [ -84538.45  ]\n",
      " [ -80381.125 ]\n",
      " [ -76093.734 ]\n",
      " [ -71674.48  ]\n",
      " [ -67121.125 ]\n",
      " [ -62431.844 ]\n",
      " [ -57604.496 ]\n",
      " [ -52637.207 ]\n",
      " [ -47527.836 ]\n",
      " [ -42274.566 ]\n",
      " [ -36875.145 ]\n",
      " [ -31327.752 ]\n",
      " [ -25630.492 ]\n",
      " [ -19780.873 ]\n",
      " [ -13777.699 ]\n",
      " [  -7618.19  ]\n",
      " [  -1300.8533]\n",
      " [   5176.411 ]\n",
      " [  11815.822 ]\n",
      " [  18619.047 ]\n",
      " [  25588.371 ]\n",
      " [  32725.6   ]\n",
      " [  40032.977 ]\n",
      " [  47512.13  ]\n",
      " [  55165.73  ]\n",
      " [  62994.83  ]\n",
      " [  71002.56  ]\n",
      " [  79189.586 ]\n",
      " [  87559.69  ]\n",
      " [  96111.74  ]\n",
      " [ 104853.71  ]\n",
      " [ 113775.914 ]\n",
      " [ 122903.85  ]\n",
      " [ 132191.48  ]\n",
      " [ 141732.    ]\n",
      " [ 151368.75  ]\n",
      " [ 161416.4   ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEICAYAAAB1f3LfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEhcFiYiIxEVBIiIicVGQiIhIXP4f14637tOBVMEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input to neural network\n",
    "t = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "epochs = 100000\n",
    "# initial learning rate\n",
    "lr = 0.01\n",
    "l_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# network architecture\n",
    "ly1 = tf.layers.dense(inputs=t, units=640, activation=tf.nn.sigmoid,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(0., .1),\n",
    "                                  bias_initializer=tf.constant_initializer(0.))\n",
    "ly2 = tf.layers.dense(inputs=ly1, units=320, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(0., .1),\n",
    "                                  bias_initializer=tf.constant_initializer(0.))\n",
    "ly3 = tf.layers.dense(inputs=ly2, units=16, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(0., .1),\n",
    "                                  bias_initializer=tf.constant_initializer(0.))\n",
    "ly4 = tf.layers.dense(inputs=ly3, units=8, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(0., .1),\n",
    "                                  bias_initializer=tf.constant_initializer(0.))\n",
    "op = tf.layers.dense(inputs=ly1, units=1, activation=None)\n",
    "\n",
    "# input range is [1,inp_length]\n",
    "inp_length = 100\n",
    "\n",
    "# computing gradient\n",
    "x_t = tf.gradients(op,t)[0]\n",
    "\n",
    "# loss function\n",
    "# equation: dx/dt = t^2 -3\n",
    "loss = tf.reduce_mean(tf.square(x_t - t*t + 3))\n",
    "\n",
    "# equation: dx/dt = 6x^2*t\n",
    "# loss = tf.reduce_mean(tf.square(x_t - 6*op[0]*op[0]*t))\n",
    "\n",
    "# optimizer\n",
    "opt = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "# opt = tf.train.GradientDescentOptimizer(learning_rate = lr).minimize(loss)\n",
    "\n",
    "# training data\n",
    "time_var = np.array([i for i in range(1,inp_length+1)])\n",
    "time_var = time_var[:, np.newaxis]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        count = 0\n",
    "        # adaptive learning rate to avoid skipping minima\n",
    "        if((i+1)%20000 == 0):\n",
    "            lr = 0.1*lr\n",
    "            print(\"learning rate:\", lr)\n",
    "        \n",
    "        inp = np.reshape(time_var, [-1,1])\n",
    "        x,_ = sess.run([op,opt], feed_dict={t: inp, l_rate: lr})\n",
    "        losses.append(loss.eval({t: time_var, l_rate: lr}))\n",
    "            \n",
    "        if(i+1 == epochs):\n",
    "            print(\"Input : \",inp,\"output : \",x,\"loss : \",loss.eval({t: time_var, l_rate: lr}))\n",
    "            print(\"Derivative: \",x_t.eval({t:inp, l_rate: lr}))\n",
    "            count = count + 1\n",
    "        if(i % 500 == 0):\n",
    "            print(\"Epoch :\", i)\n",
    "            print(\"loss : \",loss.eval({t: time_var}))\n",
    "            \n",
    "        #     print(\"x : \", x,\"loss : \",loss.eval({t: time_var, l_rate: lr}))\n",
    "        #     print(\"x_t: \",x_t.eval({t:inp, l_rate: lr}))\n",
    "\n",
    "    time_test = np.array([i for i in range(1,inp_length+1)])\n",
    "    time_test = time_test[:, np.newaxis]\n",
    "    print(op.eval({t: time_test}))\n",
    "    pred = list(op.eval({t: time_test}))\n",
    "    pred = [i[0] for i in pred]\n",
    "    derivatives = list(x_t.eval({t: time_test}))\n",
    "    derivatives = [i[0] for i in derivatives]\n",
    "    inp = [i for i in range(1,inp_length+1)]\n",
    "    plot_predicted_fn(inp, pred, derivatives)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pytorch and keras implementations are not complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ve9-XWLlwCWG"
   },
   "outputs": [],
   "source": [
    "# using pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "K8qClcuX8DJq",
    "outputId": "13e298e4-5db5-4299-96c6-0ace4b7fa17b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 1\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 2\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 3\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 4\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 5\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 6\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 7\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 8\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 9\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 10\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 11\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 12\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 13\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 14\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 15\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 16\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 17\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 18\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 19\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 20\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 21\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 22\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 23\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 24\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 25\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 26\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 27\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 28\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 29\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 30\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 31\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 32\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 33\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 34\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 35\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 36\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 37\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 38\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 39\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 40\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 41\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 42\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 43\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 44\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 45\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 46\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 47\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 48\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 49\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 50\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 51\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 52\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 53\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 54\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 55\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 56\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 57\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 58\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 59\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 60\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 61\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 62\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 63\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 64\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 65\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 66\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 67\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 68\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 69\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 70\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 71\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 72\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 73\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 74\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 75\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 76\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 77\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 78\n",
      "tensor([99940025.5180], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "Epoch : 79\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-014f72e93552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# if (i+1) % 100 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 1\n",
    "hidden_size = 64\n",
    "# num_classes = 10\n",
    "num_epochs = 50000\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# # MNIST dataset \n",
    "# train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "#                                            train=True, \n",
    "#                                            transform=transforms.ToTensor(),  \n",
    "#                                            download=True)\n",
    "\n",
    "# test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "#                                           train=False, \n",
    "#                                           transform=transforms.ToTensor())\n",
    "\n",
    "# # Data loader\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "#                                            batch_size=batch_size, \n",
    "#                                            shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "#                                           batch_size=batch_size, \n",
    "#                                           shuffle=False)\n",
    "\n",
    "#Train data\n",
    "\n",
    "train_data = np.array([[float(i)] for i in range(1,101)])\n",
    "# train_data = train_data[:, np.newaxis]\n",
    "train_data = torch.from_numpy(train_data)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size*2)\n",
    "        self.fc3 = nn.Linear(hidden_size*2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = self.tanh(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size).to(device)\n",
    "\n",
    "# def my_loss(output, inp):\n",
    "#     # loss = torch.mean((output - target)**2)\n",
    "#     losses = []\n",
    "#     for i in inp:\n",
    "#       x = torch.autograd.Variable(i,requires_grad=True)\n",
    "#       print(x)\n",
    "#       y = model(x.float())\n",
    "#       # print(\"in loss fn: \",y)\n",
    "#       y.backward()\n",
    "#       losses.append((x.grad - x**2 + 3) ** 2)\n",
    "    \n",
    "#     losses = torch.FloatTensor(losses)\n",
    "#     print(losses)\n",
    "#     loss = torch.mean(losses)\n",
    "#     return loss\n",
    "\n",
    "\n",
    "def my_loss(output, inp):\n",
    "    x = torch.autograd.Variable(inp,requires_grad=True)\n",
    "    y = model(x.float())\n",
    "    y.backward()\n",
    "    loss = (x.grad - x**2 + 3) ** 2\n",
    "    # print(x.grad)\n",
    "    return loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "# total_step = len(train_loader)\n",
    "total_step = 100\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch :\", epoch)\n",
    "    # for i, inp in enumerate(train_loader):  \n",
    "    for inp in train_data:    \n",
    "        # Forward pass\n",
    "        # i = i.reshape(-1,1).to(device)\n",
    "        # print(inp, type(inp))\n",
    "        outputs = model(inp.float())\n",
    "        # print(outputs)\n",
    "        # loss = criterion(outputs, inp.float())\n",
    "        \n",
    "        loss = my_loss(outputs, inp)\n",
    "        \n",
    "        # print(loss.item())\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # if (i+1) % 100 == 0:\n",
    "        #     print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "        #            .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    print(loss)\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in test_loader:\n",
    "#         images = images.reshape(-1, 28*28).to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# # Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DkfxnVyJE45J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "id": "9dkrYiWBE43O",
    "outputId": "d53a2414-d2aa-4576-daf7-920e6dfe03c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keras implementation\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ewrnhpKpNaIY"
   },
   "outputs": [],
   "source": [
    "def custom_loss(actual,pred):\n",
    "    loss = K.mean(K.sum(K.square((actual - pred )/10)))\n",
    "    grads = K.gradients(model.output, model.input)[0]\n",
    "    # loss = (grads - model.input**2 + 3) **2\n",
    "    inp = K.variable(model.input)\n",
    "    print(grads - inp**2 + 3 )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JzwUZFALE4y4",
    "outputId": "ac744367-7ebf-44f1-c53f-a51c12fa08fb"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-3db53c392707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# compile the keras model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# fit the keras model on the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 345\u001b[0;31m                                                 sample_weight, mask)\n\u001b[0m\u001b[1;32m    346\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \"\"\"\n\u001b[1;32m    427\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in Theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-dd3055d353bb>\u001b[0m in \u001b[0;36mcustom_loss\u001b[0;34m(actual, pred)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# loss = (grads - model.input**2 + 3) **2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(value, dtype, name, constraint)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m   def _variable_v2_call(cls,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m                         shape=None):\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2517\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2519\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1686\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m           shape=shape)\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1851\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minitial_value_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m             raise ValueError(\"initial_value must have a shape specified: %s\" %\n\u001b[0;32m-> 1853\u001b[0;31m                              self._initial_value)\n\u001b[0m\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m         \u001b[0;31m# If 'initial_value' makes use of other variables, make sure we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: initial_value must have a shape specified: Tensor(\"dense_46_input:0\", shape=(?, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "train_data = np.array([[float(i)] for i in range(1,101)])\n",
    "# train_data = train_data[:, np.newaxis]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1, activation='tanh'))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# compile the keras model\n",
    "model.compile(loss=custom_loss, optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "X = train_data\n",
    "hist = model.fit(X, X, epochs=10, batch_size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YhiA6PF42gSv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SDE_DNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
